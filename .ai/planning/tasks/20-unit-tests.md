# Task 7.1: Test Infrastructure & Coverage Verification

## Overview

Set up comprehensive testing infrastructure, establish testing standards, verify overall test coverage across the codebase, and ensure all components have been properly tested according to the test plan. This task focuses on test infrastructure and verification, NOT writing individual unit tests (those should be written alongside each implementation task).

## Priority

**P1 (High)** - Required for production readiness

## Dependencies

- All implementation tasks (1-19) - Each should include their own tests
- Task 1.1: Environment Setup

## Objectives

1. Set up pytest configuration and standards
2. Create shared test fixtures and utilities
3. Configure coverage reporting and thresholds
4. Verify test coverage across all modules (>80% target)
5. Identify and document coverage gaps
6. Set up CI/CD test integration
7. Create test execution scripts
8. Document testing patterns and best practices
9. Verify test quality (not just quantity)
10. Create test coverage dashboard

## Technical Context

### What This Task IS About
- âœ… Setting up pytest configuration
- âœ… Creating shared test fixtures (conftest.py)
- âœ… Configuring coverage tools and reports
- âœ… Verifying overall test coverage
- âœ… Identifying gaps in test coverage
- âœ… Documenting testing standards
- âœ… Setting up CI/CD test integration

### What This Task IS NOT About
- âŒ Writing unit tests for services (done in Tasks 7, 8, 9, etc.)
- âŒ Writing unit tests for utilities (done in Tasks 12, etc.)
- âŒ Writing unit tests for models (done in Task 6)
- âŒ Writing tests from scratch

**Important**: Each implementation task (1-19) should include comprehensive unit tests for the code it creates. This task verifies that all tests are in place and meet quality standards.

### Testing Framework
- **pytest**: Main testing framework
- **pytest-cov**: Coverage reporting
- **pytest-asyncio**: Async test support
- **pytest-mock**: Mocking utilities
- **faker**: Generate test data

### Test Organization
```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ services/         # Tests from Tasks 7-14
â”‚   â”œâ”€â”€ utils/            # Tests from Task 12
â”‚   â”œâ”€â”€ models/           # Tests from Task 6
â”‚   â””â”€â”€ repositories/     # Tests from Tasks 6-9
â”œâ”€â”€ conftest.py           # Shared fixtures (THIS TASK)
â”œâ”€â”€ fixtures/             # Test data utilities (THIS TASK)
â”‚   â””â”€â”€ sample_data.py
â””â”€â”€ coverage.xml          # Generated by coverage tools
```

### Coverage Goals
- Overall: >80%
- Critical paths: >95%
- Services: >85%
- Utilities: >90%

## Acceptance Criteria

- [ ] pytest configuration established with proper settings
- [ ] Shared test fixtures created (conftest.py)
- [ ] Coverage reporting configured (terminal, HTML, XML)
- [ ] All modules have >80% test coverage
- [ ] Critical paths have >95% coverage
- [ ] Coverage gaps are identified and documented
- [ ] Test execution scripts created
- [ ] CI/CD integration configured
- [ ] Testing best practices documented
- [ ] Test quality verified (not just quantity)
- [ ] Coverage reports generated and accessible

## Implementation Steps

### 1. Set Up pytest Configuration

Create `pyproject.toml` additions:

```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "-v",
    "--strict-markers",
    "--cov=google_contacts_cisco",
    "--cov-report=term-missing",
    "--cov-report=html",
    "--cov-report=xml",
    "--cov-fail-under=80",
]
markers = [
    "unit: Unit tests",
    "integration: Integration tests",
    "e2e: End-to-end tests",
    "slow: Slow running tests",
]

[tool.coverage.run]
source = ["google_contacts_cisco"]
omit = [
    "*/tests/*",
    "*/conftest.py",
    "*/__init__.py",
    "*/main.py",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
    "@abstractmethod",
]
```

### 2. Create Shared Test Fixtures

Create `tests/conftest.py`:

```python
"""Pytest configuration and shared fixtures.

This file provides reusable test fixtures that can be used across all tests.
Individual test files should import fixtures from here.
"""
import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, Session
from sqlalchemy.pool import StaticPool
from uuid import uuid4
from datetime import datetime, UTC

from google_contacts_cisco.models.base import Base
from google_contacts_cisco.models.contact import Contact
from google_contacts_cisco.models.phone_number import PhoneNumber
from google_contacts_cisco.models.email_address import EmailAddress


@pytest.fixture(scope="session")
def engine():
    """Create in-memory SQLite engine for testing.
    
    This fixture is session-scoped, so the engine is created once
    and reused across all tests in the session.
    """
    engine = create_engine(
        "sqlite:///:memory:",
        connect_args={"check_same_thread": False},
        poolclass=StaticPool,
    )
    Base.metadata.create_all(engine)
    return engine


@pytest.fixture
def db_session(engine):
    """Create a fresh database session for each test.
    
    Each test gets a clean session that is rolled back after the test,
    ensuring test isolation.
    """
    SessionLocal = sessionmaker(bind=engine)
    session = SessionLocal()
    
    try:
        yield session
    finally:
        session.rollback()
        session.close()


@pytest.fixture
def sample_contact():
    """Create a sample contact for testing."""
    return Contact(
        id=uuid4(),
        resource_name="people/c123456",
        display_name="John Doe",
        given_name="John",
        family_name="Doe",
        created_at=datetime.now(UTC),
        updated_at=datetime.now(UTC),
        deleted=False
    )


@pytest.fixture
def sample_contact_with_phones(sample_contact):
    """Create a sample contact with phone numbers."""
    sample_contact.phone_numbers = [
        PhoneNumber(
            id=uuid4(),
            contact_id=sample_contact.id,
            value="+15551234567",
            display_value="(555) 123-4567",
            type="mobile",
            primary=True
        ),
        PhoneNumber(
            id=uuid4(),
            contact_id=sample_contact.id,
            value="+15559876543",
            display_value="(555) 987-6543",
            type="work",
            primary=False
        )
    ]
    return sample_contact


@pytest.fixture
def sample_contact_with_emails(sample_contact):
    """Create a sample contact with email addresses."""
    sample_contact.email_addresses = [
        EmailAddress(
            id=uuid4(),
            contact_id=sample_contact.id,
            value="john.doe@example.com",
            type="personal",
            primary=True
        ),
        EmailAddress(
            id=uuid4(),
            contact_id=sample_contact.id,
            value="john.doe@work.com",
            type="work",
            primary=False
        )
    ]
    return sample_contact


@pytest.fixture
def multiple_contacts(db_session):
    """Create multiple contacts for list testing."""
    contacts = []
    for i in range(5):
        contact = Contact(
            id=uuid4(),
            resource_name=f"people/c{i}",
            display_name=f"Contact {i}",
            given_name=f"First{i}",
            family_name=f"Last{i}"
        )
        contacts.append(contact)
        db_session.add(contact)
    
    db_session.commit()
    return contacts


@pytest.fixture
def mock_google_api_response():
    """Mock Google People API response."""
    return {
        "resourceName": "people/c123456",
        "etag": "etag123",
        "names": [{
            "displayName": "John Doe",
            "givenName": "John",
            "familyName": "Doe"
        }],
        "phoneNumbers": [{
            "value": "+15551234567",
            "formattedValue": "(555) 123-4567",
            "type": "mobile"
        }],
        "emailAddresses": [{
            "value": "john@example.com",
            "type": "personal"
        }]
    }
```

### 3. Create Test Data Utilities

Create `tests/fixtures/sample_data.py`:

```python
"""Sample test data generators.

Provides utilities for generating realistic test data.
"""
from uuid import uuid4
from datetime import datetime, UTC
from typing import List

from google_contacts_cisco.models.contact import Contact
from google_contacts_cisco.models.phone_number import PhoneNumber
from google_contacts_cisco.models.email_address import EmailAddress


def create_contact(
    display_name: str = "Test Contact",
    given_name: str = "Test",
    family_name: str = "Contact",
    **kwargs
) -> Contact:
    """Create a test contact with defaults."""
    return Contact(
        id=kwargs.get("id", uuid4()),
        resource_name=kwargs.get("resource_name", f"people/c{uuid4().hex[:8]}"),
        display_name=display_name,
        given_name=given_name,
        family_name=family_name,
        created_at=kwargs.get("created_at", datetime.now(UTC)),
        updated_at=kwargs.get("updated_at", datetime.now(UTC)),
        deleted=kwargs.get("deleted", False)
    )


def create_contact_with_phones(
    display_name: str = "Test Contact",
    phone_count: int = 2
) -> Contact:
    """Create a test contact with phone numbers."""
    contact = create_contact(display_name=display_name)
    
    for i in range(phone_count):
        phone = PhoneNumber(
            id=uuid4(),
            contact_id=contact.id,
            value=f"+1555{i:07d}",
            display_value=f"(555) {i:03d}-{i:04d}",
            type="mobile" if i == 0 else "work",
            primary=(i == 0)
        )
        contact.phone_numbers.append(phone)
    
    return contact


def create_bulk_contacts(count: int = 10) -> List[Contact]:
    """Create multiple test contacts."""
    return [
        create_contact(
            display_name=f"Contact {i}",
            given_name=f"First{i}",
            family_name=f"Last{i}"
        )
        for i in range(count)
    ]
```

### 4. Create Test Execution Scripts

Create `scripts/test.sh`:

```bash
#!/bin/bash
# Run tests with coverage

set -e

echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "Running Unit Tests"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# Run unit tests with coverage
uv run pytest tests/unit \
    -v \
    --cov=google_contacts_cisco \
    --cov-report=term-missing \
    --cov-report=html \
    --cov-report=xml \
    --cov-fail-under=80

echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "Test Results"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "âœ… All tests passed"
echo "ðŸ“Š Coverage report: htmlcov/index.html"
echo ""
```

Create `scripts/test-watch.sh`:

```bash
#!/bin/bash
# Watch for changes and run tests

uv run pytest-watch tests/unit -- -v --cov=google_contacts_cisco
```

Create `scripts/coverage-report.sh`:

```bash
#!/bin/bash
# Generate and open coverage report

uv run pytest tests/unit \
    --cov=google_contacts_cisco \
    --cov-report=html

echo "Opening coverage report..."
python -m webbrowser htmlcov/index.html
```

### 5. Create Coverage Verification Script

Create `scripts/verify-coverage.py`:

```python
"""Verify test coverage meets requirements.

This script checks that all modules have adequate test coverage
and identifies any gaps.
"""
import xml.etree.ElementTree as ET
import sys
from pathlib import Path


def parse_coverage_xml(xml_path: Path) -> dict:
    """Parse coverage.xml and extract coverage data."""
    tree = ET.parse(xml_path)
    root = tree.getroot()
    
    coverage_data = {}
    
    for package in root.findall(".//package"):
        package_name = package.get("name")
        
        for cls in package.findall("classes/class"):
            filename = cls.get("filename")
            line_rate = float(cls.get("line-rate", 0))
            coverage_pct = line_rate * 100
            
            coverage_data[filename] = coverage_pct
    
    return coverage_data


def verify_coverage(coverage_data: dict, thresholds: dict) -> list:
    """Verify coverage meets thresholds."""
    failures = []
    
    for filepath, coverage_pct in coverage_data.items():
        # Determine threshold
        threshold = 80  # Default
        
        if "services" in filepath:
            threshold = thresholds.get("services", 85)
        elif "utils" in filepath:
            threshold = thresholds.get("utils", 90)
        elif "models" in filepath:
            threshold = thresholds.get("models", 85)
        
        if coverage_pct < threshold:
            failures.append({
                "file": filepath,
                "coverage": coverage_pct,
                "threshold": threshold,
                "gap": threshold - coverage_pct
            })
    
    return failures


def main():
    """Main verification function."""
    coverage_xml = Path("coverage.xml")
    
    if not coverage_xml.exists():
        print("âŒ No coverage.xml found. Run tests first:")
        print("   uv run pytest --cov=google_contacts_cisco --cov-report=xml")
        sys.exit(1)
    
    # Parse coverage data
    coverage_data = parse_coverage_xml(coverage_xml)
    
    # Define thresholds
    thresholds = {
        "services": 85,
        "utils": 90,
        "models": 85,
        "overall": 80
    }
    
    # Verify coverage
    failures = verify_coverage(coverage_data, thresholds)
    
    if not failures:
        print("âœ… All modules meet coverage requirements!")
        return 0
    
    print(f"âŒ {len(failures)} module(s) below coverage threshold:\n")
    
    for failure in failures:
        print(f"  {failure['file']}")
        print(f"    Coverage: {failure['coverage']:.1f}%")
        print(f"    Threshold: {failure['threshold']}%")
        print(f"    Gap: {failure['gap']:.1f}%")
        print()
    
    return 1


if __name__ == "__main__":
    sys.exit(main())
```

### 6. Document Testing Standards

Create `docs/testing-standards.md`:

```markdown
# Testing Standards

## Overview

All implementation tasks must include comprehensive unit tests. This document
outlines testing standards and best practices.

## Requirements

### Coverage Targets
- **Overall**: >80%
- **Services**: >85%
- **Utilities**: >90%
- **Critical Paths**: >95%

### Test Requirements per Task

Each implementation task must:
1. Include unit tests for all code written
2. Test both success and failure paths
3. Test edge cases and boundaries
4. Include docstrings explaining what is tested
5. Use appropriate fixtures and mocking
6. Follow naming conventions (`test_*`)

### Test Structure

```python
class TestFeatureName:
    """Test the feature name functionality."""
    
    def test_success_case(self):
        """Test the typical success path."""
        # Arrange
        ...
        # Act
        ...
        # Assert
        ...
    
    def test_error_case(self):
        """Test error handling."""
        ...
    
    def test_edge_case(self):
        """Test boundary conditions."""
        ...
```

## Best Practices

1. **One Test, One Concept**: Each test should verify one thing
2. **Arrange-Act-Assert**: Structure tests clearly
3. **Descriptive Names**: Test names should explain what is tested
4. **Independent Tests**: Tests should not depend on each other
5. **Fast Tests**: Unit tests should run in milliseconds
6. **Use Fixtures**: Reuse test data via fixtures
7. **Mock External Dependencies**: Don't call real APIs in unit tests

## Running Tests

```bash
# Run all tests
uv run pytest

# Run with coverage
uv run pytest --cov=google_contacts_cisco

# Run specific test file
uv run pytest tests/unit/services/test_sync_service.py

# Run tests matching pattern
uv run pytest -k "test_phone"
```

## Coverage Reports

```bash
# Generate HTML coverage report
uv run pytest --cov=google_contacts_cisco --cov-report=html

# Verify coverage thresholds
python scripts/verify-coverage.py
```
```

## Verification

After completing this task:

1. **Verify pytest Configuration**:
   ```bash
   uv run pytest --collect-only
   ```

2. **Run All Tests**:
   ```bash
   ./scripts/test.sh
   ```

3. **Check Coverage**:
   ```bash
   python scripts/verify-coverage.py
   ```

4. **Generate Coverage Report**:
   ```bash
   ./scripts/coverage-report.sh
   ```

5. **Verify Coverage Thresholds**:
   ```bash
   uv run pytest --cov=google_contacts_cisco --cov-fail-under=80
   ```

6. **Check Test Quality**:
   - Review tests for each module
   - Ensure edge cases are covered
   - Verify error paths are tested
   - Check that mocks are used appropriately

7. **Identify Coverage Gaps**:
   ```bash
   uv run pytest --cov=google_contacts_cisco --cov-report=term-missing
   # Look for lines marked with "!" in the output
   ```

## Notes

- **Not About Writing Tests**: This task sets up infrastructure and verifies coverage
- **Tests Per Task**: Each implementation task (1-19) should include its own tests
- **Quality Over Quantity**: 80% coverage with good tests > 100% coverage with poor tests
- **Critical Paths**: Focus extra effort on testing critical functionality
- **Fast Execution**: All unit tests should run in <30 seconds total
- **CI/CD Ready**: Configuration should work in CI/CD pipelines

## Common Issues

1. **Low Coverage**: Review tasks 1-19, ensure tests were written for each
2. **Slow Tests**: Check for unnecessary database operations or API calls
3. **Flaky Tests**: Remove time-dependent assertions, ensure proper isolation
4. **Import Errors**: Verify PYTHONPATH and package structure
5. **Coverage Reporting Fails**: Ensure coverage.xml is generated correctly

## Coverage Gap Resolution

If coverage is below 80%:

1. **Identify gaps**:
   ```bash
   uv run pytest --cov=google_contacts_cisco --cov-report=term-missing
   ```

2. **Prioritize**:
   - Critical services first
   - Core utilities second
   - Edge cases third

3. **Add missing tests**:
   - Go back to the relevant implementation task
   - Add tests for uncovered code
   - Focus on meaningful coverage, not just line count

## Related Documentation

- pytest: https://docs.pytest.org/
- pytest-cov: https://pytest-cov.readthedocs.io/
- Coverage.py: https://coverage.readthedocs.io/
- Testing Best Practices: https://docs.python-guide.org/writing/tests/

## Estimated Time

4-6 hours (infrastructure setup and verification only)

**Note**: Time for writing individual unit tests is included in each implementation task (1-19).
